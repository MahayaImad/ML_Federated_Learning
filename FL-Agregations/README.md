# Syst√®me d'Agr√©gation F√©d√©r√©e (FL-Agregations)

## üìã Description

Ce projet impl√©mente diff√©rentes m√©thodes d'agr√©gation pour l'apprentissage f√©d√©r√© sur le dataset CIFAR-10. Il compare les performances de plusieurs algorithmes d'agr√©gation incluant FedAvg, FedProx, SCAFFOLD, et des m√©thodes s√©curis√©es avec confidentialit√© diff√©rentielle.

## üéØ Objectifs Acad√©miques

- √âtudier les diff√©rentes strat√©gies d'agr√©gation en apprentissage f√©d√©r√©
- Comparer les performances en environnements IID et non-IID
- Analyser l'impact de la s√©curit√© et de la confidentialit√© diff√©rentielle
- √âvaluer les co√ªts de communication et temps de convergence

## üèóÔ∏è Architecture

```
FL-Agregations/
‚îú‚îÄ‚îÄ aggregations/           # Impl√©mentations des algorithmes d'agr√©gation
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ base_aggregator.py  # Classe abstraite de base
‚îÇ   ‚îú‚îÄ‚îÄ fedavg.py          # FedAvg standard
‚îÇ   ‚îú‚îÄ‚îÄ fedprox.py         # FedProx avec r√©gularisation proximale
‚îÇ   ‚îú‚îÄ‚îÄ scaffold.py        # SCAFFOLD avec variables de contr√¥le
‚îÇ   ‚îú‚îÄ‚îÄ fedopt.py          # FedOpt (Adam, Adagrad, Yogi)
‚îÇ   ‚îî‚îÄ‚îÄ secure_aggregation.py # Agr√©gation s√©curis√©e + DP
‚îú‚îÄ‚îÄ experiments/           # Scripts d'exp√©rimentation
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ run_comparison.py  # Comparaison des m√©thodes
‚îú‚îÄ‚îÄ client.py             # Simulation des clients f√©d√©r√©s
‚îú‚îÄ‚îÄ server.py             # Serveur f√©d√©r√©
‚îú‚îÄ‚îÄ models.py             # D√©finitions des mod√®les CNN
‚îú‚îÄ‚îÄ data_preparation.py   # Pr√©paration des donn√©es CIFAR-10
‚îú‚îÄ‚îÄ config.py             # Configuration globale
‚îî‚îÄ‚îÄ main.py               # Point d'entr√©e principal
```

## üöÄ Installation

### Pr√©requis
```bash
Python >= 3.8
TensorFlow >= 2.10
NumPy >= 1.21
Scikit-learn >= 1.0
```

### Installation des d√©pendances
```bash
pip install tensorflow numpy scikit-learn matplotlib
```

## üìä Utilisation

### Entra√Ænement avec une m√©thode sp√©cifique
```bash
# FedAvg avec distribution IID
python main.py --method fedavg --iid

# FedProx avec distribution non-IID
python main.py --method fedprox

# SCAFFOLD avec mod√®le robuste
python main.py --method scaffold --model robust
```

### Comparaison de toutes les m√©thodes
```bash
# Comparaison compl√®te
python main.py --method compare --iid

# Comparaison en environnement non-IID
python main.py --method compare
```

### Options disponibles
- `--method`: `fedavg`, `fedprox`, `scaffold`, `compare`
- `--iid`: Distribution IID des donn√©es (d√©faut: non-IID)
- `--model`: `standard`, `lightweight`, `robust`

## üî¨ M√©thodes d'Agr√©gation Impl√©ment√©es

### 1. **FedAvg** (Baseline)
- Moyenne pond√©r√©e simple des mises √† jour clients
- Algorithme de r√©f√©rence en apprentissage f√©d√©r√©

### 2. **FedProx**
- Terme de r√©gularisation proximale pendant l'entra√Ænement local
- Am√©liore la robustesse en environnements h√©t√©rog√®nes

### 3. **SCAFFOLD**
- Variables de contr√¥le pour corriger la d√©rive des clients
- Acc√©l√®re la convergence en r√©duisant la variance

### 4. **FedOpt** (Adam, Adagrad, Yogi)
- Optimiseurs adaptatifs c√¥t√© serveur
- Am√©liore la convergence avec moments adaptatifs

### 5. **Agr√©gation S√©curis√©e**
- Confidentialit√© diff√©rentielle (Œµ,Œ¥)-DP
- Simulation d'agr√©gation s√©curis√©e multi-parties

## ‚öôÔ∏è Configuration

Le fichier `config.py` contient tous les hyperparam√®tres :

```python
# Donn√©es et clients
CLIENTS = 5                    # Nombre de clients
SIZE_PER_CLIENT = 3000        # √âchantillons par client

# Entra√Ænement
COMMUNICATION_ROUNDS = 50      # Tours de communication
LOCAL_EPOCHS = 3              # √âpoques locales par client
LEARNING_RATE = 0.001         # Taux d'apprentissage

# Algorithmes sp√©cifiques
FEDPROX_MU = 0.01             # R√©gularisation FedProx
DIFFERENTIAL_PRIVACY_EPSILON = 1.0  # Budget Œµ pour DP
```

## üìà M√©triques √âvalu√©es

- **Pr√©cision finale** : Performance du mod√®le global
- **Vitesse de convergence** : Rounds n√©cessaires pour converger
- **Co√ªt de communication** : Nombre total de param√®tres √©chang√©s
- **Temps d'agr√©gation** : Efficacit√© computationnelle
- **Stabilit√©** : Variance des performances

## üîí Aspects S√©curitaires

### Confidentialit√© Diff√©rentielle
- Clipping des gradients avec norme L2
- Ajout de bruit gaussien calibr√© (Œµ,Œ¥)-DP
- Protection contre l'inf√©rence d'informations priv√©es

### Robustesse
- D√©tection d'attaques par analyse des normes
- Seuil minimum de participation
- Validation des mises √† jour clients

## üìã R√©sultats Typiques

```
=== R√âSULTATS DE COMPARAISON ===
FedAvg: Pr√©cision = 0.7245
FedProx: Pr√©cision = 0.7389
SCAFFOLD: Pr√©cision = 0.7456
```

## üîß Extensions Possibles

1. **Nouveaux algorithmes** : FedNova, FedBN, LAG
2. **Attaques et d√©fenses** : Attaques byzantines, agr√©gation robuste
3. **Optimisations** : Compression, quantification
4. **H√©t√©rog√©n√©it√©** : Mod√®les diff√©rents par client

## üêõ D√©pannage

### Erreurs courantes
```bash
# Erreur de m√©moire
ResourceExhaustedError
# Solution: R√©duire BATCH_SIZE ou SIZE_PER_CLIENT
```

## üìö R√©f√©rences

1. McMahan, H. B. et al. "Communication-Efficient Learning of Deep Networks from Decentralized Data" (FedAvg)
2. Li, T. et al. "Federated Optimization in Heterogeneous Networks" (FedProx)  
3. Karimireddy, S. P. et al. "SCAFFOLD: Stochastic Controlled Averaging for Federated Learning"
4. Reddi, S. et al. "Adaptive Federated Optimization" (FedOpt)


---

*D√©velopp√© par MAHAYA IMAD dans le cadre d'un projet acad√©mique sur l'apprentissage f√©d√©r√©*