# ğŸ¯ Attaques MIA sur Apprentissage FÃ©dÃ©rÃ©

Projet acadÃ©mique pour Ã©valuer la vulnÃ©rabilitÃ© des modÃ¨les fÃ©dÃ©rÃ©s aux attaques d'infÃ©rence d'appartenance (MIA).

## ğŸš€ Installation

```bash
pip install tensorflow numpy matplotlib scikit-learn
```

## ğŸ“ Usage

### Commandes de base

```bash
# Attaque Shadow Model (risque moyen)
python main.py --dataset mnist --attack shadow_model --risk_level medium

# Attaque Threshold (risque Ã©levÃ©)
python main.py --dataset cifar --attack threshold --risk_level high --epochs 50

# Toutes les attaques
python main.py --dataset mnist --attack all --risk_level low --verbose --plot

# CPU seulement
python main.py --dataset mnist --attack shadow_model --risk_level medium --gpu -1
```

### ParamÃ¨tres disponibles

| ParamÃ¨tre | Description | Valeurs | DÃ©faut |
|-----------|-------------|---------|--------|
| `--dataset` | Dataset Ã  utiliser | `mnist`, `cifar` | **Obligatoire** |
| `--attack` | Type d'attaque MIA | `shadow_model`, `threshold`, `gradient_based`, `all` | **Obligatoire** |
| `--risk_level` | Niveau de risque | `low`, `medium`, `high` | **Obligatoire** |
| `--epochs` | Nombre d'Ã©poques | Entier | 30 |
| `--clients` | Nombre de clients FL | Entier | 5 |
| `--target_samples` | Ã‰chantillons cibles | Entier | 1000 |
| `--shadow_models` | ModÃ¨les shadow | Entier | 5 |
| `--gpu` | GPU Ã  utiliser (-1 = CPU) | Entier | 0 |
| `--verbose` | Mode dÃ©taillÃ© | Flag | False |
| `--plot` | Afficher graphiques | Flag | False |

## ğŸ“ Structure des fichiers gÃ©nÃ©rÃ©s

```
FL-MIA-Attacks/
â”œâ”€â”€ results/                             # Rapports d'attaque (.txt, .json)
â”œâ”€â”€ models/                           # ModÃ¨les cibles et shadow (.keras)
â”œâ”€â”€ visualizations/                   # Courbes ROC et mÃ©triques (.png)
â””â”€â”€ main.py                          # Script principal
```

## ğŸ¯ Types d'attaques MIA

### Shadow Model Attack
- **Principe**: EntraÃ®ner des modÃ¨les shadow pour imiter le modÃ¨le cible
- **EfficacitÃ©**: Ã‰levÃ©e, mais coÃ»teuse en ressources
- **DÃ©tection**: Analyse des probabilitÃ©s de sortie

### Threshold Attack
- **Principe**: Utiliser un seuil sur les scores de confiance
- **EfficacitÃ©**: ModÃ©rÃ©e, simple Ã  implÃ©menter
- **DÃ©tection**: BasÃ©e sur la confiance du modÃ¨le

### Gradient-Based Attack
- **Principe**: Analyser les gradients pour dÃ©tecter l'appartenance
- **EfficacitÃ©**: Variable selon l'architecture
- **DÃ©tection**: Inspection des mises Ã  jour des gradients

## ğŸ“Š Niveaux de risque

### ğŸŸ¢ Low (Faible)
- **Configuration**: Protection renforcÃ©e, DP activÃ©e
- **Objectif**: Ã‰valuer la robustesse des dÃ©fenses
- **Seuil d'alerte**: Accuracy > 0.55

### ğŸŸ¡ Medium (Moyen)
- **Configuration**: Protection standard
- **Objectif**: Cas d'usage rÃ©aliste
- **Seuil d'alerte**: Accuracy > 0.60

### ğŸ”´ High (Ã‰levÃ©)
- **Configuration**: Aucune protection
- **Objectif**: Attaque maximale pour tester vulnÃ©rabilitÃ©s
- **Seuil d'alerte**: Accuracy > 0.70

## ğŸ“ˆ MÃ©triques d'Ã©valuation

- **Attack Accuracy**: CapacitÃ© Ã  identifier les membres
- **Precision**: Vrais positifs / (Vrais + Faux positifs)
- **Recall**: Vrais positifs / (Vrais positifs + Faux nÃ©gatifs)  
- **AUC Score**: Aire sous la courbe ROC
- **Ã‰valuation du risque**: Classification automatique

## âš ï¸ Ã‰valuation du risque

| Score MIA | Niveau | InterprÃ©tation |
|-----------|--------|----------------|
| > 0.70 | ğŸ”´ Ã‰LEVÃ‰ | ModÃ¨le trÃ¨s vulnÃ©rable |
| 0.60-0.70 | ğŸŸ¡ MOYEN | Failles dÃ©tectÃ©es |
| 0.55-0.60 | ğŸŸ¢ FAIBLE | Protection acceptable |
| < 0.55 | âœ… MINIMAL | Bonne protection |

## ğŸ’¡ Conseils d'utilisation

- **Recherche dÃ©fensive?** â†’ `--risk_level low --attack all`
- **Test de vulnÃ©rabilitÃ©?** â†’ `--risk_level high --attack shadow_model`
- **Ã‰valuation rapide?** â†’ `--attack threshold --target_samples 500`

## ğŸ”’ ConsidÃ©rations Ã©thiques

âš ï¸ **Usage acadÃ©mique uniquement**
- Ã‰valuation de vulnÃ©rabilitÃ©s pour amÃ©liorer la sÃ©curitÃ©
- Respect de la confidentialitÃ© des donnÃ©es
- Pas d'utilisation malveillante

## ğŸ› DÃ©pannage

| ProblÃ¨me | Solution |
|----------|----------|
| Erreur GPU | `--gpu -1` |
| Attaque lente | `--target_samples 500 --shadow_models 3` |
| Manque mÃ©moire | `--batch_size 16` |

## ğŸ“š RÃ©fÃ©rences acadÃ©miques

1. Shokri et al. "Membership Inference Attacks Against Machine Learning Models"
2. Truex et al. "A Hybrid Approach to Privacy-Preserving Federated Learning"
3. Melis et al. "Exploiting Unintended Feature Leakage in Collaborative Learning"
4. Nasr et al. "Comprehensive Privacy Analysis of Deep Learning"
