"""
Script principal pour attaques MIA sur apprentissage fÃ©dÃ©rÃ©
Usage: python main.py --dataset mnist --attack shadow_model --risk_level high

Exemples rapides:
  python main.py --dataset mnist --attack shadow_model --risk_level medium
  python main.py --dataset cifar --attack threshold --risk_level high --epochs 50
  python main.py --dataset mnist --attack all --verbose
"""

import argparse
import os
import sys
import tensorflow as tf
import numpy as np
import matplotlib

matplotlib.use('Agg')

from data_loader import load_mia_dataset
from attacks import run_shadow_model_attack, run_threshold_attack, run_all_attacks
from utils import setup_gpu, save_attack_results, print_attack_info


def parse_arguments():
    """Parse les arguments de ligne de commande"""
    parser = argparse.ArgumentParser(
        description='Attaques MIA (Membership Inference) sur Apprentissage FÃ©dÃ©rÃ©',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation:

  ðŸ”¹ Attaque Shadow Model (risque moyen):
     python main.py --dataset mnist --attack shadow_model --risk_level medium

  ðŸ”¹ Attaque Threshold (risque Ã©levÃ©):
     python main.py --dataset cifar --attack threshold --risk_level high --epochs 50

  ðŸ”¹ Toutes les attaques:
     python main.py --dataset mnist --attack all --risk_level low --verbose

  ðŸ”¹ Ã‰valuation complÃ¨te avec graphiques:
     python main.py --dataset cifar --attack all --risk_level medium --plot --verbose
        """
    )

    # Arguments obligatoires
    parser.add_argument('--dataset', type=str, required=True,
                        choices=['mnist', 'cifar'],
                        help='Dataset Ã  utiliser (mnist: 28x28 N&B, cifar: 32x32 couleur)')

    parser.add_argument('--attack', type=str, required=True,
                        choices=['shadow_model', 'threshold', 'gradient_based', 'all'],
                        help='Type d\'attaque MIA Ã  effectuer')

    parser.add_argument('--risk_level', type=str, required=True,
                        choices=['low', 'medium', 'high'],
                        help='Niveau de risque de l\'attaque (low: faible, medium: moyen, high: Ã©levÃ©)')

    # Arguments optionnels
    parser.add_argument('--epochs', type=int, default=30,
                        help='Nombre d\'Ã©poques d\'entraÃ®nement (dÃ©faut: 30)')

    parser.add_argument('--clients', type=int, default=5,
                        help='Nombre de clients FL (dÃ©faut: 5)')

    parser.add_argument('--batch_size', type=int, default=32,
                        help='Taille des lots (dÃ©faut: 32)')

    parser.add_argument('--lr', type=float, default=0.001,
                        help='Taux d\'apprentissage (dÃ©faut: 0.001)')

    parser.add_argument('--target_samples', type=int, default=1000,
                        help='Nombre d\'Ã©chantillons cibles pour MIA (dÃ©faut: 1000)')

    parser.add_argument('--shadow_models', type=int, default=5,
                        help='Nombre de modÃ¨les shadow (dÃ©faut: 5)')

    parser.add_argument('--gpu', type=int, default=0,
                        help='GPU Ã  utiliser (-1 pour CPU, dÃ©faut: 0)')

    parser.add_argument('--seed', type=int, default=42,
                        help='Graine alÃ©atoire (dÃ©faut: 42)')

    parser.add_argument('--verbose', action='store_true',
                        help='Mode verbeux avec dÃ©tails')

    parser.add_argument('--plot', action='store_true',
                        help='Afficher les graphiques d\'analyse')

    return parser.parse_args()


def main():
    """Fonction principale d'attaque MIA"""

    # Parse des arguments
    args = parse_arguments()

    # Configuration initiale
    print("ðŸŽ¯ DÃ©marrage des attaques MIA...")
    print_attack_info(args)

    # Setup GPU
    setup_gpu(args.gpu)

    # Fixer les graines
    tf.random.set_seed(args.seed)
    np.random.seed(args.seed)

    # CrÃ©er la structure de dossiers
    os.makedirs('results', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('visualizations', exist_ok=True)

    # Charger les donnÃ©es pour MIA
    print(f"ðŸ“Š PrÃ©paration dataset pour MIA: {args.dataset.upper()}...")
    try:
        mia_data = load_mia_dataset(
            args.dataset, args.clients, args.target_samples, args.batch_size
        )
    except Exception as e:
        print(f"âŒ Erreur prÃ©paration dataset: {e}")
        return

    # ExÃ©cuter les attaques selon le type
    print(f"ðŸŽ¯ Lancement attaque: {args.attack} (risque: {args.risk_level})")

    try:
        if args.attack == 'shadow_model':
            results = run_shadow_model_attack(mia_data, args)
        elif args.attack == 'threshold':
            results = run_threshold_attack(mia_data, args)
        elif args.attack == 'gradient_based':
            results = run_gradient_based_attack(mia_data, args)
        elif args.attack == 'all':
            results = run_all_attacks(mia_data, args)

        # Afficher les rÃ©sultats d'attaque
        print("\n" + "=" * 60)
        print("ðŸŽ¯ RÃ‰SULTATS D'ATTAQUE MIA")
        print("=" * 60)

        for attack_name, metrics in results.items():
            accuracy = metrics.get('attack_accuracy', 0.0)
            precision = metrics.get('precision', 0.0)
            recall = metrics.get('recall', 0.0)
            auc_score = metrics.get('auc_score', 0.0)

            print(f"{attack_name:20}:")
            print(f"  Accuracy:  {accuracy:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall:    {recall:.4f}")
            print(f"  AUC Score: {auc_score:.4f}")
            print(f"  Niveau de risque: {args.risk_level.upper()}")

        # Ã‰valuation du risque global
        avg_accuracy = np.mean([m.get('attack_accuracy', 0) for m in results.values()])
        risk_assessment = assess_privacy_risk(avg_accuracy, args.risk_level)

        print(f"\nðŸ” Ã‰VALUATION DU RISQUE:")
        print(f"  Accuracy moyenne des attaques: {avg_accuracy:.4f}")
        print(f"  Niveau de risque configurÃ©: {args.risk_level.upper()}")
        print(f"  Ã‰valuation: {risk_assessment}")

        # Sauvegarder les rÃ©sultats
        save_attack_results(results, args, risk_assessment)

        print(f"\nðŸ“ RÃ©sultats sauvegardÃ©s:")
        print(f"    results/ - Rapports d'attaque dÃ©taillÃ©s")
        print(f"    models/ - ModÃ¨les cibles et shadow (.keras)")
        print(f"    visualizations/ - Courbes ROC et mÃ©triques")

    except Exception as e:
        print(f"âŒ Erreur lors de l'attaque: {e}")
        import traceback
        traceback.print_exc()


def assess_privacy_risk(attack_accuracy, risk_level):
    """Ã‰value le risque de confidentialitÃ©"""
    if attack_accuracy > 0.7:
        return "ðŸ”´ RISQUE Ã‰LEVÃ‰ - ModÃ¨le vulnÃ©rable aux attaques MIA"
    elif attack_accuracy > 0.6:
        return "ðŸŸ¡ RISQUE MOYEN - Failles de confidentialitÃ© dÃ©tectÃ©es"
    elif attack_accuracy > 0.55:
        return "ðŸŸ¢ RISQUE FAIBLE - Protection acceptable"
    else:
        return "âœ… RISQUE MINIMAL - Bonne protection de la confidentialitÃ©"


if __name__ == '__main__':
    main()